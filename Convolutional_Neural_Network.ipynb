{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-0GywYDZFpkr1sSYxNOuxbYVDfelChwI",
      "authorship_tag": "ABX9TyMSIyNgzmvyfVX7o7h/EVpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelalemamera-stonybrook/projects-sandbox/blob/main/Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rZW_6f0eG-F",
        "outputId": "9b204476-9919-4402-d5ff-5637779640f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "!pip install torchcodec\n",
        "import torchaudio.transforms as T\n",
        "import pandas as pd\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_table = pd.read_csv('/content/Jordanian_dataset_3_syllable_train.csv')\n",
        "test_table = pd.read_csv('/content/Jordanian_dataset_3_syllable_test.csv')\n",
        "dev_table = pd.read_csv('/content/Jordanian_dataset_3_syllable_dev.csv')"
      ],
      "metadata": {
        "id": "QbWlgqia3phl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def populate_audio(links):\n",
        "    os.makedirs(\"/content/drive/MyDrive/dev_audio\", exist_ok=True)\n",
        "\n",
        "    for i, link in enumerate(links):\n",
        "        substring = re.findall(r\"LL.*\", link)[0]\n",
        "        src = f\"/content/drive/MyDrive/ajp/{substring}\"\n",
        "        dst = f\"/content/drive/MyDrive/dev_audio/audio{i}.wav\"\n",
        "        shutil.copy(src, dst)\n"
      ],
      "metadata": {
        "id": "SzfYhbL8ryOe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "populate_audio(list(dev_table['audio_urls']))"
      ],
      "metadata": {
        "id": "faIfs9SGswAc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/Train_audio"
      ],
      "metadata": {
        "id": "rivR8sBSntpO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/American\\ Visa.jpg /content/drive/MyDrive/Train_audio/copied.jpg"
      ],
      "metadata": {
        "id": "LEo_vCCSoUwo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, samplerate = torchaudio.load('/content/drive/MyDrive/Train_audio/audio1.wav')\n",
        "print(f\"waveform shape: {waveform.shape}\", f\"samplerate: {samplerate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-LD81Mx0DPT",
        "outputId": "189dde95-8171-4c23-f629-3a1ed6519528"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waveform shape: torch.Size([1, 57972]) samplerate: 44100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_transformed(urls):\n",
        "  '''\n",
        "  '''\n",
        "  pad_size = 0\n",
        "  max_samplerate = 0\n",
        "  waveform_list_by_batch = []\n",
        "  for batch in urls:\n",
        "    audio_list = []\n",
        "    for url in batch:\n",
        "      substring = re.findall(r\"LL.*\", url)[0]\n",
        "      waveform, samplerate = torchaudio.load(f\"/content/drive/MyDrive/ajp/{substring}\")\n",
        "      waveform = torch.reshape(waveform, (-1,))\n",
        "      audio_list.append((waveform, samplerate))\n",
        "    waveform_list_by_batch.append(audio_list)\n",
        "  for batch in waveform_list_by_batch:\n",
        "    for tup in batch:\n",
        "      length = tup[0].shape[0]\n",
        "      samplert = tup[1]\n",
        "      if length > pad_size:\n",
        "        pad_size = length\n",
        "      if samplert > max_samplerate:\n",
        "        max_samplerate = samplert\n",
        "  specgram_tensors_by_batch = []\n",
        "  for batch in waveform_list_by_batch:\n",
        "    padded_audio_list = []\n",
        "    for tup in batch:\n",
        "      audio_tensor = tup[0]\n",
        "      difference = pad_size - audio_tensor.shape[0]\n",
        "      pad = torch.zeros((difference))\n",
        "      audio_tensor = torch.cat((audio_tensor, pad))\n",
        "      padded_audio_list.append(audio_tensor)\n",
        "    padded_audio_tuple = tuple(padded_audio_list)\n",
        "    waveform_tensors = torch.vstack(padded_audio_tuple)\n",
        "    spec_gram = T.LFCC(max_samplerate)\n",
        "    spec_gram_tensors = spec_gram(waveform_tensors)\n",
        "    specgram_tensors_by_batch.append(spec_gram_tensors)\n",
        "  return specgram_tensors_by_batch\n"
      ],
      "metadata": {
        "id": "SOFlhwxEBqaJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_generated_dataset():\n",
        "  '''\n",
        "  '''\n",
        "  list_of_urls = [list(train_table['audio_urls']), list(test_table['audio_urls']), list(dev_table['audio_urls'])]\n",
        "  spec_gram_tensors_by_batch = generate_transformed(list_of_urls)\n",
        "  torch.save(spec_gram_tensors_by_batch[0], '/content/train_X.pt')\n",
        "  torch.save(spec_gram_tensors_by_batch[1], '/content/test_X.pt')\n",
        "  torch.save(spec_gram_tensors_by_batch[2], '/content/dev_X.pt')\n",
        "  torch.save(torch.tensor(list(train_table['stress'])), '/content/train_Y.pt')\n",
        "  torch.save(torch.tensor(list(test_table['stress'])), '/content/test_Y.pt')\n",
        "  torch.save(torch.tensor(list(dev_table['stress'])), '/content/dev_Y.pt')"
      ],
      "metadata": {
        "id": "gjtfPl77ZbIG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_generated_dataset()"
      ],
      "metadata": {
        "id": "dlE91lhfhLOR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.load('/content/train_X.pt')\n",
        "Y_train = torch.load('/content/train_Y.pt')\n",
        "X_dev = torch.load('/content/dev_X.pt')"
      ],
      "metadata": {
        "id": "ZE2IJ0cP4QW9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_dev.shape)\n",
        "a = torch.tensor(train_table['stress'])\n",
        "torch.sum(torch.eq(a, Y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp37xbAT4a82",
        "outputId": "c7431a3d-bd96-4eed-f1a3-ba8b37390b6b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([412, 40, 438]) torch.Size([52, 40, 438])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(412)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class conv_net(nn.Module):\n",
        "  '''\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 =\n",
        "    self.conv2 =\n",
        "    self.pool1 =\n",
        "    self.conv3 =\n",
        "    self.conv4 =\n",
        "    self.linear =\n",
        "\n",
        "  def forward():\n",
        "\n"
      ],
      "metadata": {
        "id": "kwV-hbameP1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, input_data, target_data):\n",
        "  '''\n",
        "  '''"
      ],
      "metadata": {
        "id": "S1ifRu6Peuux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}